{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Define the model class"
      ],
      "metadata": {
        "id": "aZjKCout_c1d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YU7FReO_Rh2"
      },
      "outputs": [],
      "source": [
        "# always depends :(\n",
        "\n",
        "class PerfectModel(nn.Module)\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  self.module = None\n",
        "\n",
        "  def forward(self):\n",
        "    pass\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Definition"
      ],
      "metadata": {
        "id": "U5RNyKWi_zVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCHS = 500\n",
        "PATIENCE = 50\n",
        "\n",
        "# Architecture\n",
        "HIDDEN_LAYERS = 1        # Hidden layers\n",
        "HIDDEN_SIZE = 256        # Neurons per layer\n",
        "MODEL_TYPE = 'LSTM'      # RNN, LSTM, or GRU\n",
        "BIDIRECTIONAL = False    # Use bidirectional RNN layers\n",
        "\n",
        "# Regularisation\n",
        "DROPOUT_RATE = 0         # Dropout probability\n",
        "L1_LAMBDA = 0            # L1 penalty\n",
        "L2_LAMBDA = 0            # L2 penalty\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "jmT22D8y_2Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All the boilerplate code for model training"
      ],
      "metadata": {
        "id": "60dfpIBj_4FN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, scaler, device, l1_lambda=0, l2_lambda=0):\n",
        "    \"\"\"\n",
        "    Perform one complete training epoch through the entire training dataset.\n",
        "    Adapted for Regression (Forecasting).\n",
        "    Calculates and reports RMSE.\n",
        "    Optimizes on the provided criterion (assumed to be MSELoss).\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        criterion (nn.Module): Loss function (e.g., MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): Lambda for L1 regularization\n",
        "        l2_lambda (float): Lambda for L2 regularization\n",
        "\n",
        "    Returns:\n",
        "        float: average_rmse - Training RMSE for this epoch\n",
        "    \"\"\"\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    running_mse_loss = 0.0\n",
        "\n",
        "    # Iterate through training batches\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        # Move data to device (GPU/CPU)\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Clear gradients from previous step\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Forward pass with mixed precision (if CUDA available)\n",
        "        with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "            predictions = model(inputs)\n",
        "            # Calculate the loss (e.G., MSE)\n",
        "            loss = criterion(predictions, targets)\n",
        "\n",
        "            # Add L1 and L2 regularization\n",
        "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "            l2_norm = sum(p.pow(2).sum() for p in model.parameters())\n",
        "            loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
        "\n",
        "        # Backward pass with gradient scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # Accumulate metrics\n",
        "        # We store the *squared* error (MSE) from the loss function\n",
        "        running_mse_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_mse = running_mse_loss / len(train_loader.dataset)\n",
        "    epoch_rmse = np.sqrt(epoch_mse) # Convert final MSE to RMSE for reporting\n",
        "\n",
        "    return epoch_rmse\n",
        "\n",
        "\n",
        "def validate_one_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Perform one complete validation epoch through the entire validation dataset.\n",
        "    Adapted for Regression (Forecasting).\n",
        "    Calculates and reports RMSE.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to evaluate (must be in eval mode)\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        criterion (nn.Module): Loss function used to calculate validation loss (e.g., MSELoss)\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "\n",
        "    Returns:\n",
        "        float: average_rmse - Validation RMSE for this epoch\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "\n",
        "    running_mse_loss = 0.0\n",
        "\n",
        "    # Disable gradient computation for validation\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            # Move data to device\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass with mixed precision (if CUDA available)\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
        "                predictions = model(inputs)\n",
        "                # Calculate the loss (e.g., MSE)\n",
        "                loss = criterion(predictions, targets)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            running_mse_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    # Calculate epoch metrics\n",
        "    epoch_mse = running_mse_loss / len(val_loader.dataset)\n",
        "    epoch_rmse = np.sqrt(epoch_mse) # Convert final MSE to RMSE for reporting\n",
        "\n",
        "    return epoch_rmse\n",
        "\n",
        "\n",
        "def log_metrics_to_tensorboard(writer, epoch, train_rmse, val_rmse, model):\n",
        "    \"\"\"\n",
        "    Log training metrics and model parameters to TensorBoard for visualization.\n",
        "    Adapted for Regression (Forecasting) metrics (RMSE only).\n",
        "\n",
        "    Args:\n",
        "        writer (SummaryWriter): TensorBoard SummaryWriter object for logging\n",
        "        epoch (int): Current epoch number (used as x-axis in TensorBoard plots)\n",
        "        train_rmse (float): Training RMSE for this epoch\n",
        "        val_rmse (float): Validation RMSE for this epoch\n",
        "        model (nn.Module): The neural network model (for logging weights/gradients)\n",
        "    \"\"\"\n",
        "    # Log scalar metrics\n",
        "    writer.add_scalar('RMSE/Training', train_rmse, epoch)\n",
        "    writer.add_scalar('RMSE/Validation', val_rmse, epoch)\n",
        "\n",
        "    # Log model parameters and gradients\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            # Check if the tensor is not empty before adding a histogram\n",
        "            if param.numel() > 0:\n",
        "                writer.add_histogram(f'{name}/weights', param.data, epoch)\n",
        "            if param.grad is not None:\n",
        "                # Check if the gradient tensor is not empty before adding a histogram\n",
        "                if param.grad.numel() > 0:\n",
        "                    if torch.isfinite(param.grad).all():\n",
        "                        writer.add_histogram(f'{name}/gradients', param.grad.data, epoch)\n",
        "\n",
        "\n",
        "def fit(model, train_loader, val_loader, epochs, criterion, optimizer, scaler, device,\n",
        "        l1_lambda=0, l2_lambda=0, patience=0, scheduler=None, # Added scheduler parameter\n",
        "        evaluation_metric=\"val_rmse\", mode='min', # Monitors val_rmse and minimizes\n",
        "        restore_best_weights=True, writer=None, verbose=10, experiment_name=\"\"):\n",
        "    \"\"\"\n",
        "    Train the neural network model on the training data and validate on the validation data.\n",
        "    Adapted for Regression (Forecasting), using RMSE as the sole metric.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The neural network model to train\n",
        "        train_loader (DataLoader): PyTorch DataLoader containing training data batches\n",
        "        val_loader (DataLoader): PyTorch DataLoader containing validation data batches\n",
        "        epochs (int): Number of training epochs\n",
        "        criterion (nn.Module): Loss function (e.g., MSELoss)\n",
        "        optimizer (torch.optim): Optimization algorithm (e.g., Adam, SGD)\n",
        "        scaler (GradScaler): PyTorch's gradient scaler for mixed precision training\n",
        "        device (torch.device): Computing device ('cuda' for GPU, 'cpu' for CPU)\n",
        "        l1_lambda (float): L1 regularization coefficient (default: 0)\n",
        "        l2_lambda (float): L2 regularization coefficient (default: 0)\n",
        "        patience (int): Number of epochs to wait for improvement before early stopping (default: 0)\n",
        "        scheduler (torch.optim.lr_scheduler, optional): Learning rate scheduler (default: None)\n",
        "        evaluation_metric (str): Metric to monitor for early stopping (default: \"val_rmse\")\n",
        "        mode (str): 'max' for maximizing the metric, 'min' for minimizing (default: 'min')\n",
        "        restore_best_weights (bool): Whether to restore model weights from best epoch (default: True)\n",
        "        writer (SummaryWriter, optional): TensorBoard SummaryWriter object for logging (default: None)\n",
        "        verbose (int, optional): Frequency of printing training progress (default: 10)\n",
        "        experiment_name (str, optional): Experiment name for saving models (default: \"\")\n",
        "\n",
        "    Returns:\n",
        "        tuple: (model, training_history) - Trained model and metrics history\n",
        "    \"\"\"\n",
        "\n",
        "    # Ensure models directory exists\n",
        "    if (patience > 0 or restore_best_weights) and not os.path.exists(\"models\"):\n",
        "        os.makedirs(\"models\")\n",
        "        print(\"Created 'models' directory for saving model checkpoints.\")\n",
        "\n",
        "    model_path = os.path.join(\"models\", f\"{experiment_name}_model.pt\")\n",
        "\n",
        "    # Initialize metrics tracking\n",
        "    training_history = {\n",
        "        'train_rmse': [], 'val_rmse': [], 'lr': []\n",
        "    }\n",
        "\n",
        "    # Configure early stopping if patience is set\n",
        "    if patience > 0:\n",
        "        patience_counter = 0\n",
        "        best_metric = float('inf') if mode == 'min' else float('-inf')\n",
        "        best_epoch = 0\n",
        "\n",
        "    print(f\"Training {epochs} epochs...\")\n",
        "\n",
        "    # Main training loop: iterate through epochs\n",
        "    for epoch in range(1, epochs + 1):\n",
        "\n",
        "        # Forward pass through training data, compute gradients, update weights\n",
        "        train_rmse = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, scaler, device, l1_lambda, l2_lambda\n",
        "        )\n",
        "\n",
        "        # Evaluate model on validation data without updating weights\n",
        "        val_rmse = validate_one_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        # Step the scheduler if provided (typically after validation)\n",
        "        if scheduler is not None:\n",
        "            if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                scheduler.step(val_rmse)\n",
        "            else:\n",
        "                scheduler.step()\n",
        "\n",
        "\n",
        "        # Store metrics for plotting and analysis\n",
        "        training_history['train_rmse'].append(train_rmse)\n",
        "        training_history['val_rmse'].append(val_rmse)\n",
        "        training_history['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "\n",
        "        # Write metrics to TensorBoard for visualization\n",
        "        if writer is not None:\n",
        "            log_metrics_to_tensorboard(\n",
        "                writer, epoch, train_rmse, val_rmse, model\n",
        "            )\n",
        "            # Log learning rate\n",
        "            writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], epoch)\n",
        "\n",
        "\n",
        "        # Print progress every N epochs or on first epoch\n",
        "        if verbose > 0:\n",
        "            if epoch % verbose == 0 or epoch == 1:\n",
        "                print(f\"Epoch {epoch:3d}/{epochs} | \"\n",
        "                      f\"Train: RMSE={train_rmse:.4f} | \"\n",
        "                      f\"Val: RMSE={val_rmse:.4f} | \"\n",
        "                      f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "\n",
        "        # Early stopping logic: monitor metric and save best model\n",
        "        if patience > 0:\n",
        "            # We monitor the metric specified in 'evaluation_metric' (default: 'val_rmse')\n",
        "            current_metric = training_history[evaluation_metric][-1]\n",
        "            is_improvement = (current_metric < best_metric) if mode == 'min' else (current_metric > best_metric)\n",
        "\n",
        "            if is_improvement:\n",
        "                best_metric = current_metric\n",
        "                best_epoch = epoch\n",
        "                torch.save(model.state_dict(), model_path)\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= patience:\n",
        "                    print(f\"Early stopping triggered after {epoch} epochs.\")\n",
        "                    break\n",
        "\n",
        "    # Restore best model weights if early stopping was used\n",
        "    if restore_best_weights and patience > 0:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(model_path))\n",
        "            print(f\"Best model restored from epoch {best_epoch} with {evaluation_metric} {best_metric:.4f}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: Could not find best model checkpoint at {model_path}. Using last model.\")\n",
        "\n",
        "    # Save final model if no early stopping\n",
        "    if patience == 0:\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    # Close TensorBoard writer\n",
        "    if writer is not None:\n",
        "        writer.close()\n",
        "\n",
        "    return model, training_history"
      ],
      "metadata": {
        "id": "-bcorTYC_7AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model_name..."
      ],
      "metadata": {
        "id": "e3_iRFy2ACak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer with L2 regularization\n",
        "optimizer = torch.optim.AdamW(rnn_model.parameters(), lr=LEARNING_RATE, weight_decay=L2_LAMBDA)\n",
        "\n",
        "# Enable mixed precision training for GPU acceleration\n",
        "scaler = torch.amp.GradScaler(enabled=(device.type == 'cuda'))"
      ],
      "metadata": {
        "id": "sK5A4zxBAFaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Train model and track training history\n",
        "# Define the learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',        # Monitor a metric that should be minimized (RMSE)\n",
        "    factor=0.1,        # Factor by which the learning rate will be reduced\n",
        "    patience=max(10,PATIENCE//2),       # Number of epochs with no improvement after which learning rate will be reduced\n",
        "    min_lr=1e-6        # Minimum learning rate\n",
        ")\n",
        "\n",
        "rnn_model, training_history = fit(\n",
        "    model=rnn_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=EPOCHS,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scaler=scaler,\n",
        "    device=device,\n",
        "    l1_lambda=L1_LAMBDA,\n",
        "    l2_lambda=L2_LAMBDA,\n",
        "    patience=PATIENCE,\n",
        "    evaluation_metric=\"val_rmse\",\n",
        "    mode='min',\n",
        "    restore_best_weights=True,\n",
        "    writer=writer,  # Set to writer if you want TensorBoard logging\n",
        "    verbose=1,\n",
        "    experiment_name=\"direct_lstm_forecaster_50\",\n",
        "    scheduler=scheduler # Pass the scheduler to the fit function\n",
        ")"
      ],
      "metadata": {
        "id": "FTt6ZhukAOe3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}