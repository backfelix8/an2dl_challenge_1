{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# All the imports"
      ],
      "metadata": {
        "id": "my_kGAl08NO8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72yi-pkV8BdS"
      },
      "outputs": [],
      "source": [
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "\n",
        "# Set environment variables before importing modules\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "# Import necessary modules\n",
        "import logging\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Set seeds for random number generators in NumPy and Python\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "torch.manual_seed(SEED)\n",
        "from torch import nn\n",
        "# from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "logs_dir = \"tensorboard\"\n",
        "!pkill -f tensorboard\n",
        "%load_ext tensorboard\n",
        "!mkdir -p models\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Import other libraries\n",
        "import copy\n",
        "import shutil\n",
        "from itertools import product\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure plot display settings\n",
        "sns.set(font_scale=1.4)\n",
        "sns.set_style('white')\n",
        "plt.rc('font', size=14)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading (change the dataset though)"
      ],
      "metadata": {
        "id": "yLRlC5z48j1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set environment variables for Air Quality dataset\n",
        "os.environ[\"DATASET_NAME\"] = \"[your_dataset_name]\"\n",
        "os.environ[\"DATASET_URL\"] = \"[your_dataset_url]\"\n",
        "\n",
        "# Check if Air Quality dataset exists, download and unzip if not\n",
        "if not os.path.exists(os.environ[\"DATASET_NAME\"]):\n",
        "    print(\"Downloading [Dataset Name] dataset...\")\n",
        "    !gdown -q ${DATASET_URL} -O ${DATASET_NAME}\n",
        "    print(\"[Dataset Name] dataset downloaded!\")\n",
        "else:\n",
        "    print(\"[Dataset Name] dataset already downloaded. Using cached data.\")"
      ],
      "metadata": {
        "id": "QdajAEUc8o3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Datset Exploration"
      ],
      "metadata": {
        "id": "S3_jVVm99LnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define column names for the dataset\n",
        "column_names = [list_of_column_names]\n",
        "\n",
        "# Read the dataset into a DataFrame with specified column names\n",
        "df = pd.read_csv('[csv_name]', header=None, names=column_names)\n",
        "\n",
        "# Remove rows with any missing values\n",
        "df.dropna(axis=0, how='any', inplace=True)\n",
        "\n",
        "# Print the shape of the DataFrame\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "\n",
        "# Display the first 10 rows of the DataFrame\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "DiabOZA59Omk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset-Specific Exploration"
      ],
      "metadata": {
        "id": "nbIIqSJc97K-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Always depends on the dataset... you will need to do that yourself :("
      ],
      "metadata": {
        "id": "B9Eb2o49-ADU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Split"
      ],
      "metadata": {
        "id": "cIqEi-FH-Fgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sizes for validation and test sets\n",
        "val_size = 2850\n",
        "test_size = 2850\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "X_train_raw = dataset.iloc[:-val_size-test_size]\n",
        "X_val_raw = dataset.iloc[-val_size-test_size:-test_size]\n",
        "X_test_raw = dataset.iloc[-test_size:]\n",
        "\n",
        "# Print the shapes of the split datasets\n",
        "print(f\"Train set shape: {X_train_raw.shape}\")\n",
        "print(f\"Validation set shape: {X_val_raw.shape}\")\n",
        "print(f\"Test set shape: {X_test_raw.shape}\")\n",
        "\n",
        "# Normalise data using training set statistics\n",
        "X_min = X_train_raw.min()\n",
        "X_max = X_train_raw.max()\n",
        "\n",
        "# Apply min-max normalisation\n",
        "X_train_raw = (X_train_raw - X_min) / (X_max - X_min)\n",
        "X_val_raw = (X_val_raw - X_min) / (X_max - X_min)\n",
        "X_test_raw = (X_test_raw - X_min) / (X_max - X_min)\n",
        "\n",
        "# Define target labels as the column names of the dataset\n",
        "TARGET_LABELS = dataset.columns"
      ],
      "metadata": {
        "id": "2lM_Jlzo-UQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for timeseries, you need to add a step here to convert the data into an actual time series\n",
        "# otherwise use the _raw data directly"
      ],
      "metadata": {
        "id": "tkrjuHEr-p-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert numpy arrays to PyTorch datasets (pairs features with labels)\n",
        "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"
      ],
      "metadata": {
        "id": "7Pmqwc8y-htP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loader"
      ],
      "metadata": {
        "id": "RunrJCzr-806"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_loader(ds, batch_size, shuffle, drop_last):\n",
        "    # Determine optimal number of worker processes for data loading\n",
        "    cpu_cores = os.cpu_count() or 2\n",
        "    num_workers = max(2, min(4, cpu_cores))\n",
        "\n",
        "    # Create DataLoader with performance optimizations\n",
        "    return DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,  # Faster GPU transfer\n",
        "        pin_memory_device=\"cuda\" if torch.cuda.is_available() else \"\",\n",
        "        prefetch_factor=4,  # Load 4 batches ahead\n",
        "    )"
      ],
      "metadata": {
        "id": "cs_4chrU-994"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders with different settings for each phase\n",
        "train_loader = make_loader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
        "val_loader   = make_loader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "test_loader  = make_loader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "\n",
        "for xb, yb in train_loader:\n",
        "    print(\"Features batch shape:\", xb.shape)\n",
        "    print(\"Labels batch shape:\", yb.shape)\n",
        "    break # Stop after getting one batch"
      ],
      "metadata": {
        "id": "LgrUxMd7_BdS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}